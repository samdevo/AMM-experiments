{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial utility: tensor([2.0000e+03, 2.4000e+05, 2.8500e+06, 1.0000e+01, 3.0000e+09])\n",
      "Step 0, Utility: -0.0, Relu Loss: 0.0, Difference Loss: 0.0\n",
      "Step 1000, Utility: 0.29858988523483276, Relu Loss: 0.0, Difference Loss: 1.404762952006422e-05\n",
      "Step 2000, Utility: 0.4773241877555847, Relu Loss: 0.0, Difference Loss: 1.403839087288361e-05\n",
      "Step 3000, Utility: 0.5095955729484558, Relu Loss: 0.0, Difference Loss: 1.5832585631869733e-05\n",
      "Step 4000, Utility: 0.6049559116363525, Relu Loss: 0.0, Difference Loss: 1.4150849892757833e-05\n",
      "Step 5000, Utility: 0.6222225427627563, Relu Loss: 0.0, Difference Loss: 1.4084327631280757e-05\n",
      "Step 6000, Utility: 0.6215496063232422, Relu Loss: 0.0, Difference Loss: 1.665454874455463e-05\n",
      "Step 7000, Utility: 0.6352892518043518, Relu Loss: 0.0, Difference Loss: 1.3767748896498233e-05\n",
      "Step 8000, Utility: 0.6383957862854004, Relu Loss: 0.0, Difference Loss: 1.4310629921965301e-05\n",
      "Step 9000, Utility: 0.6401913166046143, Relu Loss: 0.0, Difference Loss: 1.4059258319321088e-05\n",
      "Step 10000, Utility: 0.641201376914978, Relu Loss: 0.0, Difference Loss: 1.4130719137028791e-05\n",
      "Step 11000, Utility: 0.6418237686157227, Relu Loss: 0.0, Difference Loss: 1.4090952390688471e-05\n",
      "Step 12000, Utility: 0.642220139503479, Relu Loss: 0.0, Difference Loss: 1.4090173863223754e-05\n",
      "Step 13000, Utility: 0.6424766778945923, Relu Loss: 0.0, Difference Loss: 1.40926085805404e-05\n",
      "Step 14000, Utility: 0.5009613633155823, Relu Loss: 0.0, Difference Loss: 2.2074353182688355e-05\n",
      "Step 15000, Utility: 0.635391354560852, Relu Loss: 0.0, Difference Loss: 1.34573219838785e-05\n",
      "Step 16000, Utility: 0.6427119970321655, Relu Loss: 0.0, Difference Loss: 1.392067952110665e-05\n",
      "Step 17000, Utility: 0.6426995992660522, Relu Loss: 0.0, Difference Loss: 1.3885308362659998e-05\n",
      "Step 18000, Utility: 0.6424078941345215, Relu Loss: 0.0, Difference Loss: 1.3769616998615675e-05\n",
      "Step 19000, Utility: 0.6429591178894043, Relu Loss: 0.0, Difference Loss: 1.411663197359303e-05\n",
      "Optimized adjustments (Delta):\n",
      "tensor([[ -1.0613,  -6.9852,   7.4941],\n",
      "        [-15.9343,  24.1718, -13.6591],\n",
      "        [  0.5176,  -3.6177,   6.9524],\n",
      "        [ -3.4026,   1.2010,   0.8946],\n",
      "        [ 19.8805, -14.7699,  -1.6821]])\n",
      "Original token quantities:\n",
      "tensor([[  10,   20,   10],\n",
      "        [  60,   40,  100],\n",
      "        [ 100,  150,  190],\n",
      "        [   5,    1,    2],\n",
      "        [1000, 1500, 2000]])\n",
      "New token quantities:\n",
      "tensor([[8.9387e+00, 1.3015e+01, 1.7494e+01],\n",
      "        [4.4066e+01, 6.4172e+01, 8.6341e+01],\n",
      "        [1.0052e+02, 1.4638e+02, 1.9695e+02],\n",
      "        [1.5974e+00, 2.2010e+00, 2.8946e+00],\n",
      "        [1.0199e+03, 1.4852e+03, 1.9983e+03]])\n",
      "price ratios:\n",
      "[0.68681365 0.68668383 0.6866789  0.72572434 0.68668187]\n",
      "new utilities:\n",
      "tensor([2.0352e+03, 2.4415e+05, 2.8980e+06, 1.0177e+01, 3.0270e+09])\n",
      "utility changes:\n",
      "tensor([0.0176, 0.0173, 0.0168, 0.0177, 0.0090])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n = 5  # Number of AMMs\n",
    "k = 3  # Number of tokens\n",
    "\n",
    "quantities = [[10, 20, 10], [60, 40, 100], [100, 150, 190], [5, 1, 2], [1000, 1500, 2000]]  # Example initial quantities for each AMM\n",
    "initial_quantities = torch.tensor(quantities, requires_grad=False)\n",
    "\n",
    "# Define Delta for the first n-1 AMMs only, since the last one will be derived\n",
    "Delta = torch.zeros((n-1, k), requires_grad=True)\n",
    "\n",
    "def calc_full_delta(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    last_row_adjustment = -torch.sum(Delta, axis=0, keepdims=True)\n",
    "    # Combine adjustments to get the full Delta matrix\n",
    "    return torch.cat((Delta, last_row_adjustment), axis=0)\n",
    "\n",
    "# Objective function considering the new Delta definition\n",
    "def utilities(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    full_delta = calc_full_delta(Delta)\n",
    "    new_quantities = initial_quantities + full_delta\n",
    "    products = new_quantities.prod(dim=1)  # Product of token quantities in each AMM\n",
    "    return products\n",
    "\n",
    "initial_utility = utilities(Delta).detach()\n",
    "\n",
    "print(f\"Initial utility: {initial_utility}\")\n",
    "\n",
    "\n",
    "def utility_pct_changes(Delta):\n",
    "    utes = utilities(Delta)\n",
    "    pct_change = (utes - initial_utility) / initial_utility\n",
    "    return pct_change\n",
    "\n",
    "def utility(Delta):\n",
    "    return utility_pct_changes(Delta).sum()\n",
    "\n",
    "\n",
    "def difference_between_utility_changes(Delta):\n",
    "    # penalize differences between utility changes (ideally they should be equal)\n",
    "    utes = utility_pct_changes(Delta)\n",
    "    diffs = utes - utes.mean()\n",
    "    return diffs.abs().sum()\n",
    "\n",
    "# Optimization setup\n",
    "optimizer = optim.Adam([Delta], lr=0.05)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate utility\n",
    "    loss = -10*utility(Delta)  # Negative sign for maximization\n",
    "\n",
    "    # penalty for nonnegative values of new quantities\n",
    "    relu_loss = torch.sum(torch.relu(-initial_quantities - calc_full_delta(Delta)))\n",
    "    loss += 10000*relu_loss\n",
    "\n",
    "    # penalty for differences between utility changes (ideally they should be equal)\n",
    "    # difference_loss = difference_between_utility_changes(Delta)\n",
    "    difference_loss = utility_pct_changes(Delta).var()\n",
    "    loss += 10000 * difference_loss\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Utility: {-loss.item()}, Relu Loss: {relu_loss}, Difference Loss: {difference_loss}\")\n",
    "\n",
    "# After optimization, calculate the full Delta including the last row\n",
    "full_delta = calc_full_delta(Delta)\n",
    "\n",
    "print(\"Optimized adjustments (Delta):\")\n",
    "print(full_delta.data)\n",
    "print(\"Original token quantities:\")\n",
    "print(initial_quantities)\n",
    "print(\"New token quantities:\")\n",
    "data = (initial_quantities + full_delta).data\n",
    "print(data)\n",
    "data_np = data.numpy()\n",
    "\n",
    "price_ratio1 = data_np[0, 0] / data_np[0, 1]\n",
    "price_ratio2 = data_np[1, 0] / data_np[1, 1]\n",
    "\n",
    "\n",
    "price_ratios = data_np[:, 0] / data_np[:, 1]\n",
    "\n",
    "print(\"price ratios:\")\n",
    "print(price_ratios)\n",
    "\n",
    "print(\"new utilities:\")\n",
    "print(utilities(Delta).data)\n",
    "\n",
    "print(\"utility changes:\")\n",
    "print(utility_pct_changes(Delta).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial utility: tensor([  2000., 240000.,   1000.])\n",
      "Step 0, Utility: -0.0, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -0.0\n",
      "Step 1000, Utility: 32718.3828125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -32.7183837890625\n",
      "Step 2000, Utility: 163021.125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -163.0211181640625\n",
      "Step 3000, Utility: 215167.71875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -215.167724609375\n",
      "Step 4000, Utility: 344442.875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -344.44287109375\n",
      "Step 5000, Utility: 504137.9375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -504.137939453125\n",
      "Step 6000, Utility: 488778.3125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -488.7783203125\n",
      "Step 7000, Utility: 678534.9375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -678.534912109375\n",
      "Step 8000, Utility: 680282.5, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -680.282470703125\n",
      "Step 9000, Utility: 693789.4375, negative loss: 0.0, Relu Loss: 0.4527587890625, Difference Loss: -698.3170166015625\n",
      "Step 10000, Utility: 709808.8125, negative loss: 0.0, Relu Loss: 1.0672607421875, Difference Loss: -720.4814453125\n",
      "Step 11000, Utility: 744317.625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -744.317626953125\n",
      "Step 12000, Utility: 756988.0625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -756.988037109375\n",
      "Step 13000, Utility: 809680.0625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -809.6800537109375\n",
      "Step 14000, Utility: 845412.75, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -845.4127197265625\n",
      "Step 15000, Utility: 886260.375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -886.2603759765625\n",
      "Step 16000, Utility: 933797.5, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -933.7974853515625\n",
      "Step 17000, Utility: 987827.375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -987.827392578125\n",
      "Step 18000, Utility: 1042839.375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -1042.83935546875\n",
      "Step 19000, Utility: 897504.0, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -897.5040283203125\n",
      "Optimized adjustments (Delta):\n",
      "tensor([[  1.4334,  -9.0163,   5.9443],\n",
      "        [ -2.7521,   8.2696, -11.9136],\n",
      "        [  1.3187,   0.7467,   5.9693]])\n",
      "Original token quantities:\n",
      "tensor([[ 10,  20,  10],\n",
      "        [ 60,  40, 100],\n",
      "        [ 10,  10,  10]])\n",
      "New token quantities:\n",
      "tensor([[11.4334, 10.9837, 15.9443],\n",
      "        [57.2479, 48.2696, 88.0864],\n",
      "        [11.3187, 10.7467, 15.9693]])\n",
      "price ratios:\n",
      "[1.0409433 1.1860029 1.053226 ]\n",
      "original utilities:\n",
      "tensor([  2000., 240000.,   1000.])\n",
      "new utilities:\n",
      "tensor([  2002.3040, 243412.3906,   1942.4707])\n",
      "utility changes:\n",
      "tensor([0.0012, 0.0142, 0.9425])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n = 3  # Number of AMMs\n",
    "k = 3  # Number of tokens\n",
    "\n",
    "# quantities = [[10, 20, 10], [60, 40, 100], [100, 150, 190], [5, 1, 2], [1000, 1500, 2000]]  # Example initial quantities for each AMM\n",
    "quantities = [[10, 20, 10], [60, 40, 100], [10, 10, 10]]  # Example initial quantities for each AMM\n",
    "\n",
    "initial_quantities = torch.tensor(quantities, requires_grad=False)\n",
    "\n",
    "# Define Delta for the first n-1 AMMs only, since the last one will be derived\n",
    "Delta = torch.zeros((n-1, k), requires_grad=True)\n",
    "\n",
    "def calc_full_delta(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    last_row_adjustment = -torch.sum(Delta, axis=0, keepdims=True)\n",
    "    # Combine adjustments to get the full Delta matrix\n",
    "    return torch.cat((Delta, last_row_adjustment), axis=0)\n",
    "\n",
    "# Objective function considering the new Delta definition\n",
    "def utilities(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    full_delta = calc_full_delta(Delta)\n",
    "    new_quantities = initial_quantities + full_delta\n",
    "    products = new_quantities.prod(dim=1)  # Product of token quantities in each AMM\n",
    "    return products\n",
    "\n",
    "initial_utility = utilities(Delta).detach()\n",
    "\n",
    "print(f\"Initial utility: {initial_utility}\")\n",
    "\n",
    "\n",
    "def utility_pct_changes(Delta):\n",
    "    utes = utilities(Delta)\n",
    "    pct_change = (utes - initial_utility) / initial_utility\n",
    "    return pct_change\n",
    "\n",
    "def utility(Delta):\n",
    "    return utility_pct_changes(Delta).sum()\n",
    "\n",
    "def utility2(Delta):\n",
    "    new_utilities = utilities(Delta)\n",
    "    utility_change = new_utilities - initial_utility\n",
    "    # first n-1 rows: relu penalty for negative values. sum of the relus\n",
    "    relu_penalty = torch.relu(-utility_change[:-1]).sum()\n",
    "    # last row: maximize utility change\n",
    "    utility_change_penalty = -utility_change[-1]\n",
    "    return relu_penalty, utility_change_penalty\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def difference_between_utility_changes(Delta):\n",
    "    # penalize differences between utility changes (ideally they should be equal)\n",
    "    utes = utility_pct_changes(Delta)\n",
    "    diffs = utes - utes.mean()\n",
    "    return diffs.abs().sum()\n",
    "\n",
    "# Optimization setup\n",
    "optimizer = optim.Adam([Delta], lr=0.03)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate utility\n",
    "    # loss = -10*utility(Delta)  # Negative sign for maximization\n",
    "    loss = 0\n",
    "    # # penalty for nonnegative values of new quantities\n",
    "    relu_loss1 = torch.sum(torch.relu(-initial_quantities - calc_full_delta(Delta)))\n",
    "    loss += 10000000*relu_loss1\n",
    "\n",
    "    # # penalty for differences between utility changes (ideally they should be equal)\n",
    "    # # difference_loss = difference_between_utility_changes(Delta)\n",
    "    # difference_loss = utility_pct_changes(Delta).var()\n",
    "    # loss += 10000 * difference_loss\n",
    "    relu_loss2, utility_change_penalty = utility2(Delta)\n",
    "    loss += 1000*utility_change_penalty\n",
    "\n",
    "    loss += 10000*relu_loss2\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Utility: {-loss.item()}, negative loss: {relu_loss1}, Relu Loss: {relu_loss2}, Difference Loss: {utility_change_penalty}\")\n",
    "\n",
    "# After optimization, calculate the full Delta including the last row\n",
    "full_delta = calc_full_delta(Delta)\n",
    "\n",
    "print(\"Optimized adjustments (Delta):\")\n",
    "print(full_delta.data)\n",
    "print(\"Original token quantities:\")\n",
    "print(initial_quantities)\n",
    "print(\"New token quantities:\")\n",
    "data = (initial_quantities + full_delta).data\n",
    "print(data)\n",
    "data_np = data.numpy()\n",
    "\n",
    "price_ratio1 = data_np[0, 0] / data_np[0, 1]\n",
    "price_ratio2 = data_np[1, 0] / data_np[1, 1]\n",
    "\n",
    "\n",
    "price_ratios = data_np[:, 0] / data_np[:, 1]\n",
    "\n",
    "print(\"price ratios:\")\n",
    "print(price_ratios)\n",
    "\n",
    "print(\"original utilities:\")\n",
    "print(initial_utility)\n",
    "\n",
    "print(\"new utilities:\")\n",
    "print(utilities(Delta).data)\n",
    "\n",
    "print(\"utility changes:\")\n",
    "print(utility_pct_changes(Delta).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.4.2                                    \n",
      "===============================================================================\n",
      "(CVXPY) Apr 04 07:53:54 PM: Your problem has 4 variables, 4 constraints, and 0 parameters.\n",
      "(CVXPY) Apr 04 07:53:54 PM: It is compliant with the following grammars: \n",
      "(CVXPY) Apr 04 07:53:54 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Apr 04 07:53:54 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Apr 04 07:53:54 PM: Your problem is compiled with the CPP canonicalization backend.\n"
     ]
    },
    {
     "ename": "DCPError",
     "evalue": "Problem does not follow DCP rules. Specifically:\nThe following constraints are not DCP:\nPnorm(Vstack(var381, -Sum(var381, 0, True))[0, 0:3], 2) == 1.0 , because the following subexpressions are not:\n|--  Pnorm(Vstack(var381, -Sum(var381, 0, True))[0, 0:3], 2) == 1.0\nPnorm(Vstack(var381, -Sum(var381, 0, True))[1, 0:3], 2) == 1.0 , because the following subexpressions are not:\n|--  Pnorm(Vstack(var381, -Sum(var381, 0, True))[1, 0:3], 2) == 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDCPError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gs/_vr6zvrj58n08xdn7g2s6nb00000gn/T/ipykernel_6100/1806795240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Define the problem and solve it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mproblem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProblem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mECOS_BB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal B:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         data, solving_chain, inverse_data = self.get_problem_data(\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_dpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_dpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36mget_problem_data\u001b[0;34m(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m             solving_chain = self._construct_chain(\n\u001b[0m\u001b[1;32m    647\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0menforce_dpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_dpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_construct_chain\u001b[0;34m(self, solver, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mcandidate_solvers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_candidate_solvers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_candidate_solvers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_solvers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         return construct_solving_chain(self, candidate_solvers, gp=gp,\n\u001b[0m\u001b[1;32m    899\u001b[0m                                        \u001b[0menforce_dpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_dpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                                        \u001b[0mignore_dpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_dpp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/reductions/solvers/solving_chain.py\u001b[0m in \u001b[0;36mconstruct_solving_chain\u001b[0;34m(problem, candidates, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts, specified_solver)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSolvingChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreductions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mConstantSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mreductions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reductions_for_problem_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Process DPP status of the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/cvxpy/reductions/solvers/solving_chain.py\u001b[0m in \u001b[0;36m_reductions_for_problem_class\u001b[0;34m(problem, candidates, gp, solver_opts)\u001b[0m\n\u001b[1;32m    130\u001b[0m             append += (\"\\nHowever, the problem does follow DQCP rules. \"\n\u001b[1;32m    131\u001b[0m                        \"Consider calling solve() with `qcp=True`.\")\n\u001b[0;32m--> 132\u001b[0;31m         raise DCPError(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \"Problem does not follow DCP rules. Specifically:\\n\" + append)\n\u001b[1;32m    134\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDCPError\u001b[0m: Problem does not follow DCP rules. Specifically:\nThe following constraints are not DCP:\nPnorm(Vstack(var381, -Sum(var381, 0, True))[0, 0:3], 2) == 1.0 , because the following subexpressions are not:\n|--  Pnorm(Vstack(var381, -Sum(var381, 0, True))[0, 0:3], 2) == 1.0\nPnorm(Vstack(var381, -Sum(var381, 0, True))[1, 0:3], 2) == 1.0 , because the following subexpressions are not:\n|--  Pnorm(Vstack(var381, -Sum(var381, 0, True))[1, 0:3], 2) == 1.0"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Example matrix A (n x k)\n",
    "A = np.array([[0.25, 0.75, 0.5],\n",
    "              [0.5, 0.25, 0.75]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Variable matrix B (n x k), the matrix you're solving for\n",
    "B = cp.Variable((n-1, k))\n",
    "\n",
    "# last row is derived from the others such that each column sums to zero\n",
    "B = cp.vstack([B, -cp.sum(B, axis=0, keepdims=True)])\n",
    "\n",
    "\n",
    "# Auxiliary variable representing the common dot product value\n",
    "common_dot_product = cp.Variable()\n",
    "\n",
    "# Constraints\n",
    "# 1. Columns of B sum to zero\n",
    "# columns_sum_to_zero = [cp.sum(B, axis=0) == 0]\n",
    "# rows are unit vectors\n",
    "rows_are_unit_vectors = [cp.norm(B[i,:], 2) == 1 for i in range(n)]\n",
    "\n",
    "# 2. All row dot products are equal to the common dot product value\n",
    "row_dot_products_equal = [cp.sum(cp.multiply(A[i,:], B[i,:])) == common_dot_product for i in range(n)]\n",
    "\n",
    "# Combine all constraints\n",
    "constraints = row_dot_products_equal + rows_are_unit_vectors\n",
    "\n",
    "# Objective: Maximize the common dot product\n",
    "objective = cp.Maximize(common_dot_product)\n",
    "\n",
    "# Define the problem and solve it\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve(solver=cp.ECOS_BB, verbose=True)\n",
    "\n",
    "print(\"Optimal B:\", B.value)\n",
    "print(\"Common dot product value:\", common_dot_product.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal B: [[-1.73274486e+08  1.08344010e+08  1.91880289e+08]\n",
      " [-8.84315652e+07 -8.90848887e+07  2.71174599e+08]\n",
      " [ 2.61706051e+08 -1.92591216e+07 -4.63054888e+08]]\n",
      "Dot products: [1.33879531e+08 1.36893945e+08 1.49319982e+08]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example matrix A\n",
    "A = np.array([[0.25, 0.75, 0.5], [0.5, 0.25, 0.75], [0.6, 0.4, 0]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Define the objective function focusing only on optimizing the first n-1 rows of B\n",
    "def objective(B_flat):\n",
    "    B = B_flat.reshape((n-1), k)\n",
    "    # Derive the last row to ensure column sums of B are zero\n",
    "    last_row = -np.sum(B, axis=0)\n",
    "    B_complete = np.vstack([B, last_row])\n",
    "    # Calculate dot products of A and B\n",
    "    dot_products = np.sum(A * B_complete, axis=1)\n",
    "    # Objective: maximize the minimum dot product or any other suitable objective\n",
    "    return -np.min(dot_products)\n",
    "\n",
    "# No constraints on the row sums of B in this setup\n",
    "\n",
    "\n",
    "\n",
    "# Initial guess for B (only for the first n-1 rows)\n",
    "B_initial = np.random.rand((n-1) * k)\n",
    "\n",
    "# Solve the optimization problem\n",
    "result = minimize(objective, B_initial, method='SLSQP')\n",
    "\n",
    "if result.success:\n",
    "    # Reshape the result to match the first n-1 rows of B\n",
    "    B_optimized = result.x.reshape((n-1), k)\n",
    "    # Calculate the last row based on the optimized first n-1 rows\n",
    "    last_row = -np.sum(B_optimized, axis=0)\n",
    "    # Combine to get the complete B\n",
    "    B_complete = np.vstack([B_optimized, last_row])\n",
    "    print(\"Optimal B:\", B_complete)\n",
    "    # Optionally, calculate the uniform dot products as an additional check\n",
    "    dot_products = np.sum(A * B_complete, axis=1)\n",
    "    print(\"Dot products:\", dot_products)\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal B: [[-1.22166149e+14  2.57922534e+14 -1.19448102e+14]\n",
      " [ 1.22166149e+14 -2.57922534e+14  1.19448102e+14]]\n",
      "Objective value: 86188518002819.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.03176312e+14, 8.61885180e+13])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example matrix A\n",
    "A = np.array([[0.25, 0.75, 0.5], [0.5, 0.25, 0.75]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Objective function: Negative sum of dot products to maximize the sum\n",
    "def objective(B_flat):\n",
    "    B = B_flat.reshape(n, k)\n",
    "    dot_products = np.sum(A * B, axis=1)\n",
    "    # We try to maximize the minimum dot product\n",
    "    return -np.min(dot_products)\n",
    "\n",
    "# Constraints\n",
    "# Columns of B sum to zero\n",
    "def constraint_columns_sum_to_zero(B_flat):\n",
    "    B = B_flat.reshape(n, k)\n",
    "    return np.sum(B, axis=0)\n",
    "\n",
    "# Initial guess\n",
    "B_initial = np.random.rand(n*k)\n",
    "\n",
    "# Constraints dictionary\n",
    "con_columns = {'type': 'eq', 'fun': constraint_columns_sum_to_zero}\n",
    "\n",
    "# Optimize\n",
    "result = minimize(objective, B_initial, constraints=[con_columns], method='SLSQP')\n",
    "\n",
    "B_optimal = result.x.reshape(n, k)\n",
    "\n",
    "print(\"Optimal B:\", B_optimal)\n",
    "print(\"Objective value:\", -result.fun)  # Remember we minimized the negative sum\n",
    "\n",
    "(B_optimal * A).sum(axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
