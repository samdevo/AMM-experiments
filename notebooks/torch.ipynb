{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial utility: tensor([2.0000e+03, 2.4000e+05, 2.8500e+06, 1.0000e+01, 3.0000e+09])\n",
      "Step 0, Utility: -0.0, Relu Loss: 0.0, Difference Loss: 0.0\n",
      "Step 1000, Utility: 0.29859021306037903, Relu Loss: 0.0, Difference Loss: 1.4047721379029099e-05\n",
      "Step 2000, Utility: 0.4773259460926056, Relu Loss: 0.0, Difference Loss: 1.4062849004403688e-05\n",
      "Step 3000, Utility: 0.5666531324386597, Relu Loss: 0.0, Difference Loss: 1.4637943422712851e-05\n",
      "Step 4000, Utility: 0.6049853563308716, Relu Loss: 0.0, Difference Loss: 1.4106562957749702e-05\n",
      "Step 5000, Utility: 0.6222254633903503, Relu Loss: 0.0, Difference Loss: 1.4081449990044348e-05\n",
      "Step 6000, Utility: 0.5339349508285522, Relu Loss: 0.0, Difference Loss: 2.8870463211205788e-05\n",
      "Step 7000, Utility: 0.6357769966125488, Relu Loss: 0.0, Difference Loss: 1.401455756422365e-05\n",
      "Step 8000, Utility: 0.6384745836257935, Relu Loss: 0.0, Difference Loss: 1.4243267287383787e-05\n",
      "Step 9000, Utility: 0.6401942372322083, Relu Loss: 0.0, Difference Loss: 1.4069683857087512e-05\n",
      "Step 10000, Utility: 0.6411945819854736, Relu Loss: 0.0, Difference Loss: 1.4091440789343324e-05\n",
      "Step 11000, Utility: 0.6418271064758301, Relu Loss: 0.0, Difference Loss: 1.4091125194681808e-05\n",
      "Step 12000, Utility: 0.6421900987625122, Relu Loss: 0.0, Difference Loss: 1.3998213944432791e-05\n",
      "Step 13000, Utility: 0.6424360871315002, Relu Loss: 0.0, Difference Loss: 1.3982111340737902e-05\n",
      "Step 14000, Utility: 0.6426475048065186, Relu Loss: 0.0, Difference Loss: 1.408704792993376e-05\n",
      "Step 15000, Utility: 0.6407279968261719, Relu Loss: 0.0, Difference Loss: 1.3580525774159469e-05\n",
      "Step 16000, Utility: 0.6428355574607849, Relu Loss: 0.0, Difference Loss: 1.4089744581724517e-05\n",
      "Step 17000, Utility: 0.6428949236869812, Relu Loss: 0.0, Difference Loss: 1.4095671758695971e-05\n",
      "Step 18000, Utility: 0.6429297924041748, Relu Loss: 0.0, Difference Loss: 1.4091109733271878e-05\n",
      "Step 19000, Utility: 0.6429591178894043, Relu Loss: 0.0, Difference Loss: 1.4091107914282475e-05\n",
      "Optimized adjustments (Delta):\n",
      "tensor([[ -1.0612,  -6.9852,   7.4940],\n",
      "        [-15.9342,  24.1718, -13.6590],\n",
      "        [  0.5177,  -3.6177,   6.9525],\n",
      "        [ -3.4023,   1.2009,   0.8941],\n",
      "        [ 19.8800, -14.7698,  -1.6816]])\n",
      "Original token quantities:\n",
      "tensor([[  10,   20,   10],\n",
      "        [  60,   40,  100],\n",
      "        [ 100,  150,  190],\n",
      "        [   5,    1,    2],\n",
      "        [1000, 1500, 2000]])\n",
      "New token quantities:\n",
      "tensor([[8.9388e+00, 1.3015e+01, 1.7494e+01],\n",
      "        [4.4066e+01, 6.4172e+01, 8.6341e+01],\n",
      "        [1.0052e+02, 1.4638e+02, 1.9695e+02],\n",
      "        [1.5977e+00, 2.2009e+00, 2.8941e+00],\n",
      "        [1.0199e+03, 1.4852e+03, 1.9983e+03]])\n",
      "price ratios:\n",
      "[0.6868189  0.68668413 0.6866789  0.7259482  0.6866815 ]\n",
      "new utilities:\n",
      "tensor([2.0352e+03, 2.4415e+05, 2.8980e+06, 1.0177e+01, 3.0270e+09])\n",
      "utility changes:\n",
      "tensor([0.0176, 0.0173, 0.0168, 0.0177, 0.0090])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n = 5  # Number of AMMs\n",
    "k = 3  # Number of tokens\n",
    "\n",
    "quantities = [[10, 20, 10], [60, 40, 100], [100, 150, 190], [5, 1, 2], [1000, 1500, 2000]]  # Example initial quantities for each AMM\n",
    "initial_quantities = torch.tensor(quantities, requires_grad=False)\n",
    "\n",
    "# Define Delta for the first n-1 AMMs only, since the last one will be derived\n",
    "Delta = torch.zeros((n-1, k), requires_grad=True)\n",
    "\n",
    "def calc_full_delta(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    last_row_adjustment = -torch.sum(Delta, axis=0, keepdims=True)\n",
    "    # Combine adjustments to get the full Delta matrix\n",
    "    return torch.cat((Delta, last_row_adjustment), axis=0)\n",
    "\n",
    "# Objective function considering the new Delta definition\n",
    "def utilities(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    full_delta = calc_full_delta(Delta)\n",
    "    new_quantities = initial_quantities + full_delta\n",
    "    products = new_quantities.prod(dim=1)  # Product of token quantities in each AMM\n",
    "    return products\n",
    "\n",
    "initial_utility = utilities(Delta).detach()\n",
    "\n",
    "print(f\"Initial utility: {initial_utility}\")\n",
    "\n",
    "\n",
    "def utility_pct_changes(Delta):\n",
    "    utes = utilities(Delta)\n",
    "    pct_change = (utes - initial_utility) / initial_utility\n",
    "    return pct_change\n",
    "\n",
    "def utility(Delta):\n",
    "    return utility_pct_changes(Delta).sum()\n",
    "\n",
    "\n",
    "def difference_between_utility_changes(Delta):\n",
    "    # penalize differences between utility changes (ideally they should be equal)\n",
    "    utes = utility_pct_changes(Delta)\n",
    "    diffs = utes - utes.mean()\n",
    "    return diffs.abs().sum()\n",
    "\n",
    "# Optimization setup\n",
    "optimizer = optim.Adam([Delta], lr=0.05)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate utility\n",
    "    loss = -10*utility(Delta)  # Negative sign for maximization\n",
    "\n",
    "    # penalty for nonnegative values of new quantities\n",
    "    relu_loss = torch.sum(torch.relu(-initial_quantities - calc_full_delta(Delta)))\n",
    "    loss += 10000*relu_loss\n",
    "\n",
    "    # penalty for differences between utility changes (ideally they should be equal)\n",
    "    # difference_loss = difference_between_utility_changes(Delta)\n",
    "    difference_loss = utility_pct_changes(Delta).var()\n",
    "    loss += 10000 * difference_loss\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Utility: {-loss.item()}, Relu Loss: {relu_loss}, Difference Loss: {difference_loss}\")\n",
    "\n",
    "# After optimization, calculate the full Delta including the last row\n",
    "full_delta = calc_full_delta(Delta)\n",
    "\n",
    "print(\"Optimized adjustments (Delta):\")\n",
    "print(full_delta.data)\n",
    "print(\"Original token quantities:\")\n",
    "print(initial_quantities)\n",
    "print(\"New token quantities:\")\n",
    "data = (initial_quantities + full_delta).data\n",
    "print(data)\n",
    "data_np = data.numpy()\n",
    "\n",
    "price_ratio1 = data_np[0, 0] / data_np[0, 1]\n",
    "price_ratio2 = data_np[1, 0] / data_np[1, 1]\n",
    "\n",
    "\n",
    "price_ratios = data_np[:, 0] / data_np[:, 1]\n",
    "\n",
    "print(\"price ratios:\")\n",
    "print(price_ratios)\n",
    "\n",
    "print(\"new utilities:\")\n",
    "print(utilities(Delta).data)\n",
    "\n",
    "print(\"utility changes:\")\n",
    "print(utility_pct_changes(Delta).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial utility: tensor([  2000., 240000.,   1000.])\n",
      "Step 0, Utility: -0.0, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -0.0\n",
      "Step 1000, Utility: 32718.26171875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -32.71826171875\n",
      "Step 2000, Utility: 163019.171875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -163.0191650390625\n",
      "Step 3000, Utility: 215165.28125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -215.165283203125\n",
      "Step 4000, Utility: 344372.3125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -344.372314453125\n",
      "Step 5000, Utility: 501801.5, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -501.801513671875\n",
      "Step 6000, Utility: 489250.96875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -489.2509765625\n",
      "Step 7000, Utility: 678153.9375, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -678.1539306640625\n",
      "Step 8000, Utility: 679048.3125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -679.04833984375\n",
      "Step 9000, Utility: 697323.5, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -697.323486328125\n",
      "Step 10000, Utility: 718786.5, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -718.7864990234375\n",
      "Step 11000, Utility: 743267.5625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -743.267578125\n",
      "Step 12000, Utility: 776432.875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -776.432861328125\n",
      "Step 13000, Utility: 809406.625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -809.4066162109375\n",
      "Step 14000, Utility: 845637.3125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -845.6373291015625\n",
      "Step 15000, Utility: 886471.5625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -886.4715576171875\n",
      "Step 16000, Utility: 934094.125, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -934.0941162109375\n",
      "Step 17000, Utility: 987440.1875, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -987.440185546875\n",
      "Step 18000, Utility: 1042980.0, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -1042.97998046875\n",
      "Step 19000, Utility: 895917.625, negative loss: 0.0, Relu Loss: 0.0, Difference Loss: -895.9176025390625\n",
      "Optimized adjustments (Delta):\n",
      "tensor([[  1.4311,  -9.0187,   5.9421],\n",
      "        [ -2.7520,   8.2697, -11.9135],\n",
      "        [  1.3209,   0.7490,   5.9715]])\n",
      "Original token quantities:\n",
      "tensor([[ 10,  20,  10],\n",
      "        [ 60,  40, 100],\n",
      "        [ 10,  10,  10]])\n",
      "New token quantities:\n",
      "tensor([[11.4311, 10.9813, 15.9421],\n",
      "        [57.2480, 48.2697, 88.0865],\n",
      "        [11.3209, 10.7490, 15.9715]])\n",
      "price ratios:\n",
      "[1.040954  1.1860036 1.0532086]\n",
      "original utilities:\n",
      "tensor([  2000., 240000.,   1000.])\n",
      "new utilities:\n",
      "tensor([  2001.1802, 243413.2188,   1943.5406])\n",
      "utility changes:\n",
      "tensor([5.9009e-04, 1.4222e-02, 9.4354e-01])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "n = 3  # Number of AMMs\n",
    "k = 3  # Number of tokens\n",
    "\n",
    "# quantities = [[10, 20, 10], [60, 40, 100], [100, 150, 190], [5, 1, 2], [1000, 1500, 2000]]  # Example initial quantities for each AMM\n",
    "quantities = [[10, 20, 10], [60, 40, 100], [10, 10, 10]]  # Example initial quantities for each AMM\n",
    "\n",
    "initial_quantities = torch.tensor(quantities, requires_grad=False)\n",
    "\n",
    "# Define Delta for the first n-1 AMMs only, since the last one will be derived\n",
    "Delta = torch.zeros((n-1, k), requires_grad=True)\n",
    "\n",
    "def calc_full_delta(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    last_row_adjustment = -torch.sum(Delta, axis=0, keepdims=True)\n",
    "    # Combine adjustments to get the full Delta matrix\n",
    "    return torch.cat((Delta, last_row_adjustment), axis=0)\n",
    "\n",
    "# Objective function considering the new Delta definition\n",
    "def utilities(Delta):\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    full_delta = calc_full_delta(Delta)\n",
    "    new_quantities = initial_quantities + full_delta\n",
    "    products = new_quantities.prod(dim=1)  # Product of token quantities in each AMM\n",
    "    return products\n",
    "\n",
    "initial_utility = utilities(Delta).detach()\n",
    "\n",
    "print(f\"Initial utility: {initial_utility}\")\n",
    "\n",
    "\n",
    "def utility_pct_changes(Delta):\n",
    "    utes = utilities(Delta)\n",
    "    pct_change = (utes - initial_utility) / initial_utility\n",
    "    return pct_change\n",
    "\n",
    "def utility(Delta):\n",
    "    return utility_pct_changes(Delta).sum()\n",
    "\n",
    "def utility2(Delta):\n",
    "    new_utilities = utilities(Delta)\n",
    "    utility_change = new_utilities - initial_utility\n",
    "    # first n-1 rows: relu penalty for negative values. sum of the relus\n",
    "    relu_penalty = torch.relu(-utility_change[:-1]).sum()\n",
    "    # last row: maximize utility change\n",
    "    utility_change_penalty = -utility_change[-1]\n",
    "    return relu_penalty, utility_change_penalty\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def difference_between_utility_changes(Delta):\n",
    "    # penalize differences between utility changes (ideally they should be equal)\n",
    "    utes = utility_pct_changes(Delta)\n",
    "    diffs = utes - utes.mean()\n",
    "    return diffs.abs().sum()\n",
    "\n",
    "# Optimization setup\n",
    "optimizer = optim.Adam([Delta], lr=0.03)\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calculate utility\n",
    "    # loss = -10*utility(Delta)  # Negative sign for maximization\n",
    "    loss = 0\n",
    "    # # penalty for nonnegative values of new quantities\n",
    "    relu_loss1 = torch.sum(torch.relu(-initial_quantities - calc_full_delta(Delta)))\n",
    "    loss += 10000000*relu_loss1\n",
    "\n",
    "    # # penalty for differences between utility changes (ideally they should be equal)\n",
    "    # # difference_loss = difference_between_utility_changes(Delta)\n",
    "    # difference_loss = utility_pct_changes(Delta).var()\n",
    "    # loss += 10000 * difference_loss\n",
    "    relu_loss2, utility_change_penalty = utility2(Delta)\n",
    "    loss += 1000*utility_change_penalty\n",
    "\n",
    "    loss += 10000*relu_loss2\n",
    "    \n",
    "    # Backward pass and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Utility: {-loss.item()}, negative loss: {relu_loss1}, Relu Loss: {relu_loss2}, Difference Loss: {utility_change_penalty}\")\n",
    "\n",
    "# After optimization, calculate the full Delta including the last row\n",
    "full_delta = calc_full_delta(Delta)\n",
    "\n",
    "print(\"Optimized adjustments (Delta):\")\n",
    "print(full_delta.data)\n",
    "print(\"Original token quantities:\")\n",
    "print(initial_quantities)\n",
    "print(\"New token quantities:\")\n",
    "data = (initial_quantities + full_delta).data\n",
    "print(data)\n",
    "data_np = data.numpy()\n",
    "\n",
    "price_ratio1 = data_np[0, 0] / data_np[0, 1]\n",
    "price_ratio2 = data_np[1, 0] / data_np[1, 1]\n",
    "\n",
    "\n",
    "price_ratios = data_np[:, 0] / data_np[:, 1]\n",
    "\n",
    "print(\"price ratios:\")\n",
    "print(price_ratios)\n",
    "\n",
    "print(\"original utilities:\")\n",
    "print(initial_utility)\n",
    "\n",
    "print(\"new utilities:\")\n",
    "print(utilities(Delta).data)\n",
    "\n",
    "print(\"utility changes:\")\n",
    "print(utility_pct_changes(Delta).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Example matrix A (n x k)\n",
    "A = np.array([[0.25, 0.75, 0.5],\n",
    "              [0.5, 0.25, 0.75]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Variable matrix B (n x k), the matrix you're solving for\n",
    "B = cp.Variable((n-1, k))\n",
    "\n",
    "# last row is derived from the others such that each column sums to zero\n",
    "B = cp.vstack([B, -cp.sum(B, axis=0, keepdims=True)])\n",
    "\n",
    "\n",
    "# Auxiliary variable representing the common dot product value\n",
    "common_dot_product = cp.Variable()\n",
    "\n",
    "# Constraints\n",
    "# 1. Columns of B sum to zero\n",
    "# columns_sum_to_zero = [cp.sum(B, axis=0) == 0]\n",
    "# rows are unit vectors\n",
    "rows_are_unit_vectors = [cp.norm(B[i,:], 2) == 1 for i in range(n)]\n",
    "\n",
    "# 2. All row dot products are equal to the common dot product value\n",
    "row_dot_products_equal = [cp.sum(cp.multiply(A[i,:], B[i,:])) == common_dot_product for i in range(n)]\n",
    "\n",
    "# Combine all constraints\n",
    "constraints = row_dot_products_equal + rows_are_unit_vectors\n",
    "\n",
    "# Objective: Maximize the common dot product\n",
    "objective = cp.Maximize(common_dot_product)\n",
    "\n",
    "# Define the problem and solve it\n",
    "problem = cp.Problem(objective, constraints)\n",
    "problem.solve(solver=cp.ECOS_BB, verbose=True)\n",
    "\n",
    "print(\"Optimal B:\", B.value)\n",
    "print(\"Common dot product value:\", common_dot_product.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal B: [[-1.73274486e+08  1.08344010e+08  1.91880289e+08]\n",
      " [-8.84315652e+07 -8.90848887e+07  2.71174599e+08]\n",
      " [ 2.61706051e+08 -1.92591216e+07 -4.63054888e+08]]\n",
      "Dot products: [1.33879531e+08 1.36893945e+08 1.49319982e+08]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example matrix A\n",
    "A = np.array([[0.25, 0.75, 0.5], [0.5, 0.25, 0.75], [0.6, 0.4, 0]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Define the objective function focusing only on optimizing the first n-1 rows of B\n",
    "def objective(B_flat):\n",
    "    B = B_flat.reshape((n-1), k)\n",
    "    # Derive the last row to ensure column sums of B are zero\n",
    "    last_row = -np.sum(B, axis=0)\n",
    "    B_complete = np.vstack([B, last_row])\n",
    "    # Calculate dot products of A and B\n",
    "    dot_products = np.sum(A * B_complete, axis=1)\n",
    "    # Objective: maximize the minimum dot product or any other suitable objective\n",
    "    return -np.min(dot_products)\n",
    "\n",
    "# No constraints on the row sums of B in this setup\n",
    "\n",
    "\n",
    "\n",
    "# Initial guess for B (only for the first n-1 rows)\n",
    "B_initial = np.random.rand((n-1) * k)\n",
    "\n",
    "# Solve the optimization problem\n",
    "result = minimize(objective, B_initial, method='SLSQP')\n",
    "\n",
    "if result.success:\n",
    "    # Reshape the result to match the first n-1 rows of B\n",
    "    B_optimized = result.x.reshape((n-1), k)\n",
    "    # Calculate the last row based on the optimized first n-1 rows\n",
    "    last_row = -np.sum(B_optimized, axis=0)\n",
    "    # Combine to get the complete B\n",
    "    B_complete = np.vstack([B_optimized, last_row])\n",
    "    print(\"Optimal B:\", B_complete)\n",
    "    # Optionally, calculate the uniform dot products as an additional check\n",
    "    dot_products = np.sum(A * B_complete, axis=1)\n",
    "    print(\"Dot products:\", dot_products)\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal B: [[-1.22166149e+14  2.57922534e+14 -1.19448102e+14]\n",
      " [ 1.22166149e+14 -2.57922534e+14  1.19448102e+14]]\n",
      "Objective value: 86188518002819.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.03176312e+14, 8.61885180e+13])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Example matrix A\n",
    "A = np.array([[0.25, 0.75, 0.5], [0.5, 0.25, 0.75]])\n",
    "\n",
    "n, k = A.shape\n",
    "\n",
    "# Objective function: Negative sum of dot products to maximize the sum\n",
    "def objective(B_flat):\n",
    "    B = B_flat.reshape(n, k)\n",
    "    dot_products = np.sum(A * B, axis=1)\n",
    "    # We try to maximize the minimum dot product\n",
    "    return -np.min(dot_products)\n",
    "\n",
    "# Constraints\n",
    "# Columns of B sum to zero\n",
    "def constraint_columns_sum_to_zero(B_flat):\n",
    "    B = B_flat.reshape(n, k)\n",
    "    return np.sum(B, axis=0)\n",
    "\n",
    "# Initial guess\n",
    "B_initial = np.random.rand(n*k)\n",
    "\n",
    "# Constraints dictionary\n",
    "con_columns = {'type': 'eq', 'fun': constraint_columns_sum_to_zero}\n",
    "\n",
    "# Optimize\n",
    "result = minimize(objective, B_initial, constraints=[con_columns], method='SLSQP')\n",
    "\n",
    "B_optimal = result.x.reshape(n, k)\n",
    "\n",
    "print(\"Optimal B:\", B_optimal)\n",
    "print(\"Objective value:\", -result.fun)  # Remember we minimized the negative sum\n",
    "\n",
    "(B_optimal * A).sum(axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
