{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 Cos utility: 260.0 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.5000, 1.5000], grad_fn=<DivBackward0>)\n",
      "step 100 Cos utility: 275.0952453613281 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4416, 1.4767], grad_fn=<DivBackward0>)\n",
      "step 200 Cos utility: 292.7398681640625 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.3602, 1.4567], grad_fn=<DivBackward0>)\n",
      "step 300 Cos utility: 313.13653564453125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2458, 1.4391], grad_fn=<DivBackward0>)\n",
      "step 400 Cos utility: 336.4225769042969 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0796, 1.4232], grad_fn=<DivBackward0>)\n",
      "step 500 Cos utility: 355.5625915527344 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0050, 1.3765], grad_fn=<DivBackward0>)\n",
      "step 600 Cos utility: 371.23687744140625 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([1.2688e-03, 1.3196e+00], grad_fn=<DivBackward0>)\n",
      "step 700 Cos utility: 386.41680908203125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0019, 1.2678], grad_fn=<DivBackward0>)\n",
      "step 800 Cos utility: 401.35369873046875 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0039, 1.2205], grad_fn=<DivBackward0>)\n",
      "step 900 Cos utility: 416.1249084472656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0202, 1.1772], grad_fn=<DivBackward0>)\n",
      "step 1000 Cos utility: 419.7796936035156 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.3819, 1.1670], grad_fn=<DivBackward0>)\n",
      "step 1100 Cos utility: 419.7566833496094 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4114, 1.1670], grad_fn=<DivBackward0>)\n",
      "step 1200 Cos utility: 419.79345703125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4480, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 1300 Cos utility: 419.816650390625 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4593, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 1400 Cos utility: 419.825927734375 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4294, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 1500 Cos utility: 419.84368896484375 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4696, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 1600 Cos utility: 419.855712890625 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4576, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 1700 Cos utility: 419.8661193847656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4409, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 1800 Cos utility: 419.84405517578125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.3189, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 1900 Cos utility: 419.876220703125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.3738, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 2000 Cos utility: 419.88409423828125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.3488, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 2100 Cos utility: 419.8788146972656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4872, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 2200 Cos utility: 419.8858337402344 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4645, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 2300 Cos utility: 419.8922424316406 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.4390, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 2400 Cos utility: 419.8702087402344 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2988, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 2500 Cos utility: 419.8729553222656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2936, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 2600 Cos utility: 419.87890625 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2685, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 2700 Cos utility: 419.88568115234375 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2445, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 2800 Cos utility: 419.89129638671875 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2167, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 2900 Cos utility: 419.8963928222656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1916, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3000 Cos utility: 419.8960876464844 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1510, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3100 Cos utility: 419.90130615234375 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1206, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3200 Cos utility: 419.90020751953125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2416, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 3300 Cos utility: 419.90509033203125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.2107, 1.1668], grad_fn=<DivBackward0>)\n",
      "step 3400 Cos utility: 419.90997314453125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1774, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3500 Cos utility: 419.8907775878906 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1080, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3600 Cos utility: 419.8939514160156 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.1006, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3700 Cos utility: 419.91729736328125 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0891, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3800 Cos utility: 419.9217834472656 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0519, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 3900 Cos utility: 419.92645263671875 | Cos loss: 0 | ReLU loss: 0.0 | prices: tensor([0.0123, 1.1669], grad_fn=<DivBackward0>)\n",
      "step 4000 Cos utility: 419.93109130859375 | Cos loss: 0 | ReLU loss: 0.025177001953125 | prices: tensor([-0.0250,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4100 Cos utility: 419.9356384277344 | Cos loss: 0 | ReLU loss: 0.06723403930664062 | prices: tensor([-0.0688,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4200 Cos utility: 419.9164123535156 | Cos loss: 0 | ReLU loss: 0.10957717895507812 | prices: tensor([-0.0851,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4300 Cos utility: 419.92108154296875 | Cos loss: 0 | ReLU loss: 0.15211105346679688 | prices: tensor([-0.1210,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4400 Cos utility: 419.91510009765625 | Cos loss: 0 | ReLU loss: 0.036716461181640625 | prices: tensor([-0.0295,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4500 Cos utility: 419.92047119140625 | Cos loss: 0 | ReLU loss: 0.07877349853515625 | prices: tensor([-0.0655,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4600 Cos utility: 419.92510986328125 | Cos loss: 0 | ReLU loss: 0.12149810791015625 | prices: tensor([-0.1035,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4700 Cos utility: 419.92962646484375 | Cos loss: 0 | ReLU loss: 0.16326904296875 | prices: tensor([-0.1425,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4800 Cos utility: 419.9325256347656 | Cos loss: 0 | ReLU loss: 0.17881393432617188 | prices: tensor([-0.1601,  1.1669], grad_fn=<DivBackward0>)\n",
      "step 4900 Cos utility: 419.93218994140625 | Cos loss: 0 | ReLU loss: 0.22020339965820312 | prices: tensor([-0.1901,  1.1669], grad_fn=<DivBackward0>)\n",
      "Final Delta: tensor([[-10.0026, -19.9887],\n",
      "        [ 10.0026,  19.9887]], grad_fn=<CatBackward0>)\n",
      "Final Utility: tensor([-2.9549e-05,  4.1994e+03], grad_fn=<ProdBackward1>)\n",
      "Final Gradient: tensor([[ 1.1292e-02, -2.6169e-03],\n",
      "        [ 5.9989e+01,  7.0003e+01]])\n",
      "Final Cosine similarity integral: tensor([-10.2940, 103.2109], grad_fn=<AddBackward0>)\n",
      "Final Cosine utility: 92.9168701171875\n",
      "Final Cosine loss: 6441.6796875\n",
      "Final ReLU loss: 0.00261688232421875\n",
      "Final balances: tensor([[-2.6169e-03,  1.1292e-02],\n",
      "        [ 7.0003e+01,  5.9989e+01]], grad_fn=<AddBackward0>)\n",
      "Final utilities: tensor([-2.9549e-05,  4.1994e+03], grad_fn=<ProdBackward1>)\n",
      "final prices: tensor([-0.2318,  1.1669], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n = 2\n",
    "k = 2\n",
    "\n",
    "quantities = [[10, 20], [60, 40]]  # Example initial quantities for each AMM\n",
    "# \n",
    "# quantities = [[10, 20, 10], [60, 40, 100], [100, 150, 190], [5, 1, 2], [1000, 1500, 2000]]  # Example initial quantities for each AMM\n",
    "initial_quantities = torch.tensor(quantities, requires_grad=False)\n",
    "\n",
    "# Define Delta for the first n-1 AMMs only, since the last one will be derived\n",
    "Delta_incomplete = torch.zeros((n-1, k), requires_grad=True)\n",
    "\n",
    "def get_Delta(Delta_incomplete):\n",
    "    assert Delta_incomplete.shape == (n-1, k), f\"Delta_incomplete should have shape {(n-1, k)}\"\n",
    "    last_row_adjustment = -torch.sum(Delta_incomplete, axis=0, keepdims=True)\n",
    "    # Combine adjustments to get the full Delta matrix\n",
    "    return torch.cat((Delta_incomplete, last_row_adjustment), axis=0)\n",
    "\n",
    "# Objective function considering the new Delta definition\n",
    "def utilities(Delta):\n",
    "    assert Delta.shape == (n, k), f\"Delta should have shape {(n, k)}\"\n",
    "    # Calculate adjustments for the last AMM as the negation of the sum of the others\n",
    "    new_quantities = initial_quantities + Delta\n",
    "    products = new_quantities.prod(dim=1)  # Product of token quantities in each AMM\n",
    "    return products\n",
    "\n",
    "# get the gradient of utility w.r.t Delta\n",
    "def get_gradient(Delta):\n",
    "    assert Delta.shape == (n, k), f\"Delta should have shape {(n, k)}\"\n",
    "    utilities_per_amm = utilities(Delta)\n",
    "    # print(f\"Delta[0]: {Delta[0]}\")\n",
    "    # print(f\"utilities_per_amm[0]: {utilities_per_amm[0]}\")\n",
    "    gradients_per_amm = []\n",
    "    for i in range(n):\n",
    "        # if i < n - 1:\n",
    "        #     grad_output = torch.autograd.grad(\n",
    "        #         utilities_per_amm, \n",
    "        #         Delta, \n",
    "        #         grad_outputs=torch.ones_like(utilities_per_amm),\n",
    "        #         retain_graph=True\n",
    "        #     )\n",
    "        #     print(grad_output)\n",
    "        #     grad = grad_output[0]\n",
    "        # else:\n",
    "        #     # For the last AMM, compute gradient with respect to the negation of the sum of previous Deltas\n",
    "        #     grad = torch.autograd.grad(utilities_per_amm[i], Delta[:-1], retain_graph=True)[0]\n",
    "        #     grad = -torch.sum(grad, dim=0)  # Sum and negate to get the gradient for the last row\n",
    "        # print(grad)\n",
    "        # # HERE\n",
    "        grad_outputs = torch.zeros_like(utilities_per_amm)\n",
    "        grad_outputs[i] = 1  # Set 1 for the i-th utility, zeros elsewhere\n",
    "        grad = torch.autograd.grad(utilities_per_amm, Delta, grad_outputs=grad_outputs,\n",
    "                                retain_graph=True)[0][i]\n",
    "        gradients_per_amm.append(grad)  # Assign gradient to corresponding Delta\n",
    "        # gradients_per_amm.append(grad)\n",
    "    # print(f\"gradients_per_amm: {gradients_per_amm}\")\n",
    "    # print(f\"stacked: {torch.stack(gradients_per_amm)}\")\n",
    "    return torch.stack(gradients_per_amm)\n",
    "    # this has shape (n, k)\n",
    "\n",
    "def cosine_utility(cosine_similarities):\n",
    "    return cosine_similarities.sum()\n",
    "\n",
    "def cosine_difference_loss(cosine_similarities):\n",
    "    return cosine_similarities.var()\n",
    "\n",
    "def cosine_similarities(grads, Delta) -> torch.Tensor:\n",
    "    assert grads.shape == (n, k), f\"grads should have shape {(n, k)}\"\n",
    "    assert Delta.shape == (n, k), f\"Delta should have shape {(n, k)}\"\n",
    "    similarities = grads * Delta / torch.linalg.norm(grads, dim=1)\n",
    "    return similarities.sum(dim=1)\n",
    "    cosines = []\n",
    "    for i in range(n):\n",
    "        grad = grads[i]\n",
    "        cos_times_v = torch.dot(grad, Delta[i]) / (torch.linalg.vector_norm(grad))\n",
    "        cosines.append(cos_times_v)\n",
    "    return cosines\n",
    "\n",
    "\n",
    "def integrated_cosine_similarity(Delta):\n",
    "    N = 20\n",
    "    cosines = torch.zeros(Delta.shape[0])\n",
    "    for i in range(N):\n",
    "        curr_Delta = i/N * Delta\n",
    "        grads = get_gradient(curr_Delta)\n",
    "        cosines += cosine_similarities(grads, Delta) * (1/N)\n",
    "    return cosines\n",
    "\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam([Delta_incomplete], lr=0.02)\n",
    "\n",
    "# Training loop\n",
    "for i in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    Delta = get_Delta(Delta_incomplete)\n",
    "    # print(f\"Delta: {Delta.shape}\")\n",
    "    # print(f\"gradient: {get_gradient(Delta).shape}\")\n",
    "    # print(f\"shapes: {Delta.shape}, {initial_quantities.shape}, {get_gradient(Delta).shape}\")\n",
    "    # cos_similarities = cosine_similarities(get_gradient(Delta), Delta)\n",
    "    cos_similarities_integrated = integrated_cosine_similarity(Delta)\n",
    "    cos_utility = 1000*cos_similarities_integrated.sum()\n",
    "    cos_utility = utilities(Delta).sum() / 10\n",
    "    cos_loss = 1000*cos_similarities_integrated.var()\n",
    "    cos_loss = 0\n",
    "    relu_loss = 100*torch.sum(torch.relu(-initial_quantities - Delta))\n",
    "    loss = 0\n",
    "    loss -= cos_utility\n",
    "    loss += cos_loss\n",
    "    loss += relu_loss\n",
    "    quantities = initial_quantities + Delta\n",
    "    prices = quantities[:, 0] / quantities[:, 1]\n",
    "    # price_diff_loss = prices.var()\n",
    "    # loss += 10000*price_diff_loss\n",
    "    if i % 100 == 0:\n",
    "        print(f\"step {i} Cos utility: {cos_utility} | Cos loss: {cos_loss} | ReLU loss: {relu_loss} | prices: {prices}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # if i % 100 == 0:\n",
    "    #     print(f\"Iteration {i}, loss: {loss.item()}\")\n",
    "\n",
    "    # if i % 100 == 0:\n",
    "    #     print(f\"Utility: {utility}\")\n",
    "    #     print(f\"Delta: {Delta}\")\n",
    "    #     grads = get_gradient(Delta)\n",
    "    #     print(f\"Gradients: {grads}\")\n",
    "    #     cosines = cosine_similarities(grads, Delta)\n",
    "    #     print(f\"Cosine similarities: {cosines}\")\n",
    "    #     print(\"\")\n",
    "\n",
    "final_delta = get_Delta(Delta_incomplete)\n",
    "final_grad = get_gradient(final_delta)\n",
    "final_quantities = initial_quantities + final_delta\n",
    "print(f\"Final Delta: {final_delta}\")\n",
    "print(f\"Final Utility: {utilities(final_delta)}\")\n",
    "print(f\"Final Gradient: {final_grad}\")\n",
    "print(f\"Final Cosine similarity integral: {integrated_cosine_similarity(final_delta)}\")\n",
    "print(f\"Final Cosine utility: {cosine_utility(integrated_cosine_similarity(final_delta))}\")\n",
    "print(f\"Final Cosine loss: {cosine_difference_loss(integrated_cosine_similarity(final_delta))}\")\n",
    "print(f\"Final ReLU loss: {torch.sum(torch.relu(-initial_quantities - final_delta))}\")\n",
    "\n",
    "print(f\"Final balances: {initial_quantities + final_delta}\")\n",
    "print(f\"Final utilities: {utilities(final_delta)}\")\n",
    "\n",
    "print(f\"final prices: {final_quantities[:, 0] / final_quantities[:, 1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
